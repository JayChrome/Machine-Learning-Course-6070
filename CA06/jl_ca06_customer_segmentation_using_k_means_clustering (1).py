# -*- coding: utf-8 -*-
"""JL_CA06: Customer Segmentation using K-Means Clustering

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16WPxntIcoods7B1nw4IqNlv4Yj2ltcTr

#Section 1: Load the dataset and perform exploratory data analysis (EDA)

1a. Import the
necessary libraries (pandas, numpy, matplotlib, seaborn)

 1b. Load the dataset
using pandas and display the first few rows
"""

# Download this file from the internet to my machine 
!wget https://github.com/ArinB/MSBA-CA-Data/raw/main/CA06/Mall_Customers.csv

# Install Autoviz
!pip install autoviz
from autoviz.AutoViz_Class import AutoViz_Class

# Commented out IPython magic to ensure Python compatibility.
# import libraries 
import pandas as pd
import seaborn as sns
# %matplotlib inline

# download the dataset and save it to my  local machine
url = 'https://github.com/ArinB/MSBA-CA-Data/raw/main/CA06/Mall_Customers.csv'
df = pd.read_csv(url)
df.to_csv('Mall_Customers.csv', index=False)

# see the first few lines of the dataset

df.head()

#see the shape of the dataset

df.shape

# see the dtypes

df.info()

"""1d. Visualize the distribution of features using
histograms or boxplots
"""

# create an instance of AutoViz and pass the file path
AV = AutoViz_Class()
AV.AutoViz('Mall_Customers.csv')

# check for duplicates

df.duplicated().sum()

# check for null values 
df.isnull().sum()

# describe the data set and see attributes for each feature 
df.describe()

"""I want to use box plots to easily check for outliers """

# create a box plot to check for outliers
import seaborn as sns

sns.boxplot(x=df['Age'])

# check for outliers in the annual income column
sns.boxplot(x=df['Annual Income (k$)'])

# check for outliiers in the spending score column
sns.boxplot(x=df['Spending Score (1-100)'])

# impute the mean for the row where there was an outlier
mean_income = df.loc[df['Annual Income (k$)']<=120, 'Annual Income (k$)'].mean()
df.loc[df['Annual Income (k$)']>120, 'Annual Income (k$)'] = mean_income

# check to make sure the row was imputed and there are no more rows that have outliers for this feature  
df[df['Annual Income (k$)']>120]

# make sure the data shape is the same 
df.shape

"""Outliers were removed. Let's scale and normalize the data, but before I do that, it's important to understand if the features I'm interested in, annual income and spending score, are normally distributed"""

# view the distribution for annual income 
sns.kdeplot(df['Annual Income (k$)'], shade=True)

# view the distribution for spending score 
sns.kdeplot(df['Spending Score (1-100)'], shade=True)

"""#Section 2: Prepare the data for clustering

2b. Choose the appropriate features for
clustering (you may start with 'Annual Income' and 'Spending Score') 

2c. Create a
new DataFrame with only the selected features
"""

# Choose features that would provide the most insights, Annual income and Spending score for anlaysis.
# both of them are the most impactful variables and could provide insights around spending levels for stakeholders

x = df.loc[:, ['Annual Income (k$)', 'Spending Score (1-100)']].values

"""2a. Perform any necessary feature scaling
(StandardScaler or MinMaxScaler)
"""

# use min max scaler as the feature normalization technique since 
# the variables that we're looking at are relatively normally distributed
# fit minmaxscaler to the x variable 
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler().fit(x)
print(scaler)
MinMaxScaler()

# confirm scaler feature range 

scaler.feature_range

"""the scaler feature range is 0 and 1, which means that we were able to fit the data to the scaler, as the range should be between 0 and 1 when using minmaxscaler"""

# apply the scaling transformation to the dataset 
scaler.transform(x)

"""#Section 3: Implement k-means clustering

3a. Import the KMeans class from the
sklearn.cluster module

3b. Use the Silhouette Method to determine the optimal
number of clusters

The silhouette score measure how well each data point fits into tis assigned cluster and how different it is from other clusters. A 1 indicates that the data is well matched, while a -1 indicates that the data is poorly matched to other clusters. Higher silhouette scores equate to better clustering. 

Let's use the silhoutte score to determine the number of clusters
"""

from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans


# Create empty lists to store the silhouette scores and number of clusters
silhouette_scores = []
clusters = range(2,11)

# Loop through different numbers of clusters
for k in clusters:
    # Create a KMeans object with k clusters
    kmeans = KMeans(n_clusters=k)
    # Fit the KMeans object to the data
    cluster_labels = kmeans.fit_predict(x)
    #The silhouette_score gives the average value for all the   samples.
    #Calculating number of clusters
    silhouette_avg = silhouette_score(x, cluster_labels)
    print("For n_clusters =", k, "The average   silhoutte_score is :", silhouette_avg)
    # Append the silhouette score to the list
    silhouette_scores.append(silhouette_avg)
    
# Plot the silhouette scores vs. number of clusters
plt.plot(clusters, silhouette_scores, 'bx-')
plt.xlabel('Number of Clusters')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Analysis')
plt.show()

""" 3c. Train the KMeans model with the optimal number of
clusters 

3d. Obtain the cluster assignments for each data poin
"""

# train the model with the optimal number of clusters

kmeans = KMeans(n_clusters = 5, init = 'k-means++') #initalize the class object
label = kmeans.fit_predict(x) #return the cluster number for each of the data points
print(label)

"""# Section 4: Visualize and analyze the clusters

4a. Create a scatter plot of the selected features,
colored by cluster assignment
"""

plt.figure(figsize=(8,8))
plt.scatter(x[label == 0,0], x[label== 0,1], s=50, c='green', label='Cluster 1')
plt.scatter(x[label == 1,0], x[label== 1,1], s=50, c='yellow', label='Cluster 2')
plt.scatter(x[label == 2,0], x[label== 2,1], s=50, c='red', label='Cluster 3')
plt.scatter(x[label == 3,0], x[label== 3,1], s=50, c='purple', label='Cluster 4')
plt.scatter(x[label == 4,0], x[label== 4,1], s=50, c='blue', label='Cluster 5')
plt.scatter(kmeans.cluster_centers_ [:,0], kmeans.cluster_centers_ [:,1], s= 100, c='black', marker= '*', label='Centriods') #Plotting the centriods
plt.title('Customer groups')
plt.xlabel('Annual Income')
plt.ylabel('Spending Score (1-100)')
plt.legend()
plt.show()

"""cluster 4 has the highest degree of compactness. Most of the customers in this group have similar shopping behavior, even though there are a few that drift between the lower spending threshold. On the other hand, customers in clusters one and three have high intercluster similarity between other clusters, but are lower than cluster four when it comes to intracluster similarity. Illustrating that customers in these groups have a much more varied shopping and spending behavior than those in cluster 4. Comparatively, clusters two and five have medium intracluster similarity and medium intercluster similarity. These clusters are well made because of this."""